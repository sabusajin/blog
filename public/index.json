[{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Building scalable backend infrastructure to transform the legal experience for all\n","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eBuilding scalable backend infrastructure to transform the legal experience for all\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/clio/img1.png#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Developer II (Backend Infrastructure)"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description ","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description ","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/clio/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans Addressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods Developed a custom loss function combining CGAN loss and Dice Loss for improved image segmentation Implemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans Leveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training Utilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/pytorch-CycleGAN-and-pix2pix/tree/3D_Seg\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eMohammad Farid Azampour\u003c/strong\u003e (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRearchitected Pix2Pix, a CGAN architecture, to generate Ultrasound (US) scans from MRI scans\u003c/li\u003e\n\u003cli\u003eAddressed the challenge of structural dissimilarity between MRI and US scans due to data collection methods\u003c/li\u003e\n\u003cli\u003eDeveloped a custom loss function combining CGAN loss and Dice Loss for improved image segmentation\u003c/li\u003e\n\u003cli\u003eImplemented the loss function to encourage the generator to eliminate structural deformation in the generated US scans\u003c/li\u003e\n\u003cli\u003eLeveraged remote access to TU-Munich\u0026rsquo;s cluster computers for fast model training\u003c/li\u003e\n\u003cli\u003eUtilized TU-Munich\u0026rsquo;s Discourse forum for collaboration which drove the project to fruition\u003c/li\u003e\n\u003c/ul\u003e","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers\u003c/strong\u003e, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring\u003c/strong\u003e, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeveloped a real-time hardware event notification system in Windows kernel mode\u003c/strong\u003e, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOptimized PCIe link rate management in NVMe drivers\u003c/strong\u003e, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode\u003c/strong\u003e, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProduced comprehensive documentation and automated test suites for new storage features\u003c/strong\u003e, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring\u003c/strong\u003e, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDeveloped a real-time hardware event notification system in Windows kernel mode\u003c/strong\u003e, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOptimized PCIe link rate management in NVMe drivers\u003c/strong\u003e, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode\u003c/strong\u003e, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProduced comprehensive documentation and automated test suites for new storage features\u003c/strong\u003e, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances Standardized the training procedure for ML models, saving over 100 hours of development time for the ML team Worked on fetal head segmentation using the developed framework Gained insights into the applications of AI and Computer Vision in medical diagnosis Acquired experience in working with limited and sensitive healthcare data ","permalink":"http://localhost:1313/experience/origin-health/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eDr. Sripad Krishna Devalla\u003c/strong\u003e (co-founder and CTO at OriginHealth)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped a configuration-driven framework with data preprocessing pipeline for training deep learning models on AWS EC2 instances\u003c/li\u003e\n\u003cli\u003eStandardized the training procedure for ML models, saving over 100 hours of development time for the ML team\u003c/li\u003e\n\u003cli\u003eWorked on fetal head segmentation using the developed framework\u003c/li\u003e\n\u003cli\u003eGained insights into the applications of AI and Computer Vision in medical diagnosis\u003c/li\u003e\n\u003cli\u003eAcquired experience in working with limited and sensitive healthcare data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img1.jpeg#center\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/origin-health/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Intern"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"ðŸ”— GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation Developed a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images The model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions The neural network was later utilized for robot localization in GPS-denied environments ","permalink":"http://localhost:1313/experience/iit-madras/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/Tensorflow/tree/master/VLocNet\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGuide:\u003c/strong\u003e \u003cstrong\u003eProf. Dr. Pratyush Kumar\u003c/strong\u003e (Assistant Professor, Dept. of Computer Science, IIT Madras)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImplemented a Convolutional Neural Network (CNN) for 6-DoF Global Pose Regression and Odometry Estimation\u003c/li\u003e\n\u003cli\u003eDeveloped a deep learning model in TensorFlow to estimate the camera pose from consecutive monocular images\u003c/li\u003e\n\u003cli\u003eThe model demonstrated superior performance compared to traditional feature-based visual localization algorithms, especially in texture-less regions\u003c/li\u003e\n\u003cli\u003eThe neural network was later utilized for robot localization in GPS-denied environments\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img1.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img2.jpeg\" alt=\"\"  /\u003e\n\n\u003cimg loading=\"lazy\" src=\"/experience/iit-madras/img3.jpeg\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Computer Vision Intern"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/tumunich/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"ðŸ”— Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1Q553uslYW3Ho6P1G46SOEDxOS_VmHXfJ\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I implemented the paper \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/1502.03044\"\u003eShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u003c/a\u003e\u003c/strong\u003e. The neural network, a combination of \u003cstrong\u003eCNN\u003c/strong\u003e and \u003cstrong\u003eLSTM\u003c/strong\u003e, was trained on the \u003cstrong\u003eMS COCO\u003c/strong\u003e dataset and it learns to generate captions from images.\u003c/p\u003e\n\u003cp\u003eAs the network generates the caption, word by word, the modelâ€™s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.\n\u003cimg loading=\"lazy\" src=\"/projects/automated-image-captioning/img1.jpg\" alt=\"Attention Mechanism\"  /\u003e\n\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Add project description here!\n","permalink":"http://localhost:1313/projects/automated-image-captioning/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAdd project description here!\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"Intro In my video about How I cleared the AWS SAA Certification Exam, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my Obsidian Vault repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, Sarthak Narayan, had been working over the past 2 weeks on the project, Obsidian Publish using GitHub Action, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at notes.arkalim.org. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme ","permalink":"http://localhost:1313/projects/obsidian-publish-github-action/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn my video about \u003ca href=\"https://arkalim.org/blog/aws-saa-certification/\"\u003e\u003cstrong\u003eHow I cleared the AWS SAA Certification Exam\u003c/strong\u003e\u003c/a\u003e, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\u003c/p\u003e","title":"Obsidian Publish using GitHub Action"},{"content":"ðŸ”— GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users donâ€™t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:1313/projects/kindle-to-notion/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/kindle-to-notion\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI like reading personal improvement and mindset change type books on \u003cstrong\u003eKindle\u003c/strong\u003e e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\u003c/p\u003e\n\u003cp\u003eKindle exports the highlights as a file named \u003ccode\u003eMyClippings.txt\u003c/code\u003e. The \u003cstrong\u003eNodeJS\u003c/strong\u003e application reads the \u003ccode\u003eMyClipping.txt\u003c/code\u003e file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses \u003cstrong\u003eNotion API\u003c/strong\u003e to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\u003c/p\u003e","title":"Kindle to Notion"},{"content":"ðŸ”— View App ðŸ”— GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:1313/projects/todo-list-app/","summary":"\u003ch3 id=\"-view-app\"\u003eðŸ”— \u003ca href=\"https://arkalim-todo-list.netlify.app\"\u003eView App\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/arkalim/todo-list-app\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA to-do list web application built using \u003cstrong\u003eReact\u003c/strong\u003e that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning \u003cstrong\u003eReact\u003c/strong\u003e.\u003c/p\u003e","title":"Todo List App"},{"content":"ðŸ”— Colab Notebook ðŸ”— Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:1313/projects/face-landmarks-detection/","summary":"\u003ch3 id=\"-colab-notebook\"\u003eðŸ”— \u003ca href=\"https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb\"\u003eColab Notebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/face-landmarks-detection\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eIn this project, I trained a neural network to localize key points on faces. \u003cstrong\u003eResnet-18\u003c/strong\u003e was used as the model with some slight modifications to the input and output layer. The model was trained on the official \u003cstrong\u003eDLib Dataset\u003c/strong\u003e containing \u003cstrong\u003e6666 images\u003c/strong\u003e along with corresponding \u003cstrong\u003e68-point landmarks\u003c/strong\u003e for each face. Additionally, I wrote a custom data preprocessing pipeline in \u003cstrong\u003ePyTorch\u003c/strong\u003e to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\u003c/p\u003e","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:1313/projects/gaze-tracking-goggles/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. \u003cstrong\u003eSingle Shot Descriptor\u003c/strong\u003e, with \u003cstrong\u003eVGG16\u003c/strong\u003e as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using \u003cstrong\u003eTkInter\u003c/strong\u003e for ease of use.\u003c/p\u003e","title":"Gaze-tracking Goggles"},{"content":"ðŸ”— GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:1313/projects/openquad/","summary":"\u003ch3 id=\"-github\"\u003eðŸ”— \u003ca href=\"https://github.com/OpenQuad-RMI/openquad\"\u003eGitHub\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eThe aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a \u003cstrong\u003ePixhawk\u003c/strong\u003e flight controller with \u003cstrong\u003eRaspberry Pi\u003c/strong\u003e as a companion computer. \u003cstrong\u003eDJI Flame Wheel-450\u003c/strong\u003e is used for the quadcopter frame along with some custom mountings for adding additional components.\u003c/p\u003e","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\nðŸ”— Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:1313/projects/search-and-reconnaissance-robot/","summary":"\u003cblockquote\u003e\n\u003cp\u003ePresented in the 4th International and 19th National Conference on Machine and Mechanisms (\u003cstrong\u003eiNaCoMM 2019\u003c/strong\u003e)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePublished in the \u003cstrong\u003eSpringer 2019\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"-publication\"\u003eðŸ”— \u003ca href=\"https://www.researchgate.net/publication/343361428_Search_and_Reconnaissance_Robot_for_Disaster_Management\"\u003ePublication\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eNatural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body canâ€™t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\u003c/p\u003e","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:1313/projects/sebart-pro/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eI worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. \u003cstrong\u003eSEBART-Pro\u003c/strong\u003e is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an \u003cstrong\u003eArduino Nano\u003c/strong\u003e as the microcontroller. The robot senses the tilt using an \u003cstrong\u003eMPU-6050 (6-axis gyroscope and accelerometer)\u003c/strong\u003e and converts the values from these sensors into angles using a \u003cstrong\u003eKalman Filter\u003c/strong\u003e. It uses the \u003cstrong\u003ePID control algorithm\u003c/strong\u003e to balance on two wheels and a simple \u003cstrong\u003eConvolutional Neural Network\u003c/strong\u003e is used to recognize traffic signs.\u003c/p\u003e","title":"SEBART-Pro"},{"content":"Description Add project description here!\n","permalink":"http://localhost:1313/projects/hellloworld/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAdd project description here!\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"ðŸ”— Blog Post Description Add project description here!\n","permalink":"http://localhost:1313/projects/hellloworld/","summary":"\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/helloworld\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAdd project description here!\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"},{"content":"Hello World Welcome to my blog!\nIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\n","permalink":"http://localhost:1313/blog/helloworld/","summary":"\u003ch1 id=\"hello-world\"\u003eHello World\u003c/h1\u003e\n\u003cp\u003eWelcome to my blog!\u003c/p\u003e\n\u003cp\u003eIâ€™m excited to start sharing my day-to-day learnings, experiments, and thoughts on low-level coding and high-performance systems. Stay tuned for more technical deep dives and project updates!\u003c/p\u003e","title":"Hello World!"},{"content":"ðŸ”— Blog Post Description Add project description here!\n","permalink":"http://localhost:1313/projects/hellloworld/","summary":"\u003ch3 id=\"-blog-post\"\u003eðŸ”— \u003ca href=\"../../blog/helloworld\"\u003eBlog Post\u003c/a\u003e\u003c/h3\u003e\n\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eAdd project description here!\u003c/p\u003e","title":"Hello World"},{"content":"Description Architected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings. Engineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services. Developed a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime. Optimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware. Diagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features. Produced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development. ","permalink":"http://localhost:1313/experience/microsoft/","summary":"\u003ch2 id=\"description\"\u003eDescription\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eArchitected and delivered Windows kernel-mode NVMe Zoned Storage drivers, enabling Azure Storage to deploy QLC SSDs at scale and realize $35M+ in projected COGS savings.\u003c/li\u003e\n\u003cli\u003eEngineered user-mode libraries and kernel-mode drivers for ZNS disk operations and health monitoring, empowering Azure Storage to efficiently manage and monitor ZNS disks, supporting high-availability cloud services.\u003c/li\u003e\n\u003cli\u003eDeveloped a real-time hardware event notification system in Windows kernel mode, allowing enterprise customers to respond instantly to critical hardware changes and significantly improving system reliability and uptime.\u003c/li\u003e\n\u003cli\u003eOptimized PCIe link rate management in NVMe drivers, extending battery life by up to 30 minutes on supported Windows client hardware.\u003c/li\u003e\n\u003cli\u003eDiagnosed and resolved complex cross-layer issues spanning user and kernel mode, ensuring high reliability and performance for Windows storage features.\u003c/li\u003e\n\u003cli\u003eProduced comprehensive documentation and automated test suites for new storage features, enhancing cross-team collaboration and maintainability for Windows storage development.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Delivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs. Enhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems. ","permalink":"http://localhost:1313/experience/qualcomm/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDelivered advanced UFS driver features (UFS 3.0, 3.1, WriteProtect, Refresh, Power Management) for Windows and QNX, expanding the capabilities of automotive infotainment and ADAS platforms for global OEMs.\u003c/li\u003e\n\u003cli\u003eEnhanced system boot and power management in QNX, reducing boot-up times by 20% and low power mode exit latency by 30% on automotive systems.\u003c/li\u003e\n\u003c/ul\u003e","title":"Senior Software Engineer"},{"content":"Description Drove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms. Headed a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines. Led a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions. Supported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products. ","permalink":"http://localhost:1313/experience/intel/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDrove low power storage initiatives (UFS 2.1/3.0 boot, power management, inline hardware encryption) in Windows drivers, enabling advanced storage features for Intel client platforms.\u003c/li\u003e\n\u003cli\u003eHeaded a UFS task force to debug and resolve 20+ ship-blocking issues in one week, ensuring the successful release of Intelâ€™s first hybrid architecture product under aggressive timelines.\u003c/li\u003e\n\u003cli\u003eLed a cross-company debug effort to pinpoint a device firmware bottleneck impacting disk encryption, achieving an 80% reduction in encryption time and contributing to millions in quarterly revenue for Intelâ€™s Windows-based storage solutions.\u003c/li\u003e\n\u003cli\u003eSupported power-on/bring-up of multiple UFS client platforms with Windows driver integration, accelerating time-to-market for new Windows-compatible Intel products.\u003c/li\u003e\n\u003c/ul\u003e","title":"Software Engineer"},{"content":"Description Software Engineering intern with the Core OS team. Mostly worked on memory management issues. ","permalink":"http://localhost:1313/experience/windriver/","summary":"\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware Engineering intern with the Core OS team.\u003c/li\u003e\n\u003cli\u003eMostly worked on memory management issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/experience/16bit/img1.jpeg#center\" alt=\"\"  /\u003e\n\u003c/p\u003e","title":"Software Engineering Intern"}]